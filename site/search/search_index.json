{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Web Crawling Menggunakan Python Perkenalan Nama : Nisa Desiana NIM : 160411100129 Fakultas / Jurusan : Teknik / Teknik Informatika Perguruan Tinggi : Universitas Trunojoyo Madura Tutorial ini dibuat untuk memenuhi tugas Mata Kuliah \"Penambangan dan Pencarian Web\" yang dibimbing oleh Dosen Bapak Mulaab, S.Si., M.Kom. Apa itu Web Crawler? Web crawler merupakan suatu program yang digunakan search engine untuk menjelajahi web yang ada di internet. Web crawler biasa digunakan untuk membuat salinan sebagian atau keseluruhan halaman web. Web crawler juga digunakan untuk memperoleh data yang khusus. Crawl digunakan untuk mengambil data yang bisa berupa text, citra, audio, video, dll. Apa itu Python dan Beautifulsoup4? \u200b Python merupakan bahasa pemrograman tingkat tinggi yang dibuat oleh Guido van Rossum. Python juga dikenal dengan bahasa pemrograman yang mudah dipelajari, karena struktur sintaknya rapi dan mudah dipahami. Python memiliki banyak library(fitur) untuk menyelesaikan suatu permasalahan. Python banyak digunakan untuk membuat berbagai macam program, seperti: program CLI, Program GUI (desktop), Aplikasi Mobile, Web, IoT, Game, Program untuk Hacking, dsb. \u200b Beautiful Soup adalah library di python untuk menarik data yang ada di HTML. Sebelum saya mengenal Beautiful Soup atau teknik scraping, saya hanya meng-copy-paste data table yang ada di web lalu di pindahkan ke excel. namun proses tersebut tentu sangat tidak efisien. Selain library Beautiful Soup, ada juga scrapy yang fungsinya hampir sama. Hanya berbeda dari segi teknik pemakaian dan kebutuhan. Website yang di Crawl: http://www.tentangsinopsis.com Python 3.5 yang menggunakan library bawaannya yaitu: requests BeautifulSoup4 SQLite3 numpy csv sastrawi scikit-learn (untuk install numpy dan scipy terlebih dulu) Scikit-fuzzy (perlu untuk install numpy dan scipy terlebih dulu) Web Crawling hanya dapat dilakukan jika komputer terkoneksi dengan jaringan internet.","title":"Perkenalan"},{"location":"#web-crawling-menggunakan-python","text":"","title":"Web Crawling Menggunakan Python"},{"location":"#perkenalan","text":"Nama : Nisa Desiana NIM : 160411100129 Fakultas / Jurusan : Teknik / Teknik Informatika Perguruan Tinggi : Universitas Trunojoyo Madura Tutorial ini dibuat untuk memenuhi tugas Mata Kuliah \"Penambangan dan Pencarian Web\" yang dibimbing oleh Dosen Bapak Mulaab, S.Si., M.Kom.","title":"Perkenalan"},{"location":"#apa-itu-web-crawler","text":"Web crawler merupakan suatu program yang digunakan search engine untuk menjelajahi web yang ada di internet. Web crawler biasa digunakan untuk membuat salinan sebagian atau keseluruhan halaman web. Web crawler juga digunakan untuk memperoleh data yang khusus. Crawl digunakan untuk mengambil data yang bisa berupa text, citra, audio, video, dll.","title":"Apa itu Web Crawler?"},{"location":"#apa-itu-python-dan-beautifulsoup4","text":"\u200b Python merupakan bahasa pemrograman tingkat tinggi yang dibuat oleh Guido van Rossum. Python juga dikenal dengan bahasa pemrograman yang mudah dipelajari, karena struktur sintaknya rapi dan mudah dipahami. Python memiliki banyak library(fitur) untuk menyelesaikan suatu permasalahan. Python banyak digunakan untuk membuat berbagai macam program, seperti: program CLI, Program GUI (desktop), Aplikasi Mobile, Web, IoT, Game, Program untuk Hacking, dsb. \u200b Beautiful Soup adalah library di python untuk menarik data yang ada di HTML. Sebelum saya mengenal Beautiful Soup atau teknik scraping, saya hanya meng-copy-paste data table yang ada di web lalu di pindahkan ke excel. namun proses tersebut tentu sangat tidak efisien. Selain library Beautiful Soup, ada juga scrapy yang fungsinya hampir sama. Hanya berbeda dari segi teknik pemakaian dan kebutuhan. Website yang di Crawl: http://www.tentangsinopsis.com Python 3.5 yang menggunakan library bawaannya yaitu: requests BeautifulSoup4 SQLite3 numpy csv sastrawi scikit-learn (untuk install numpy dan scipy terlebih dulu) Scikit-fuzzy (perlu untuk install numpy dan scipy terlebih dulu) Web Crawling hanya dapat dilakukan jika komputer terkoneksi dengan jaringan internet.","title":"Apa itu Python dan Beautifulsoup4?"},{"location":"clustering/","text":"K Mean Clustering Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: Tentukan jumlah klaster yang ingin dibentuk dan tetapkan pusat cluster k. Menggunakan jarak euclidean kemudian hitung setiap data ke pusat cluster Kelompokkan data ke dalam cluster dengan jarak yang paling pendek dengan persamaan Hitung pusat cluster yang baru menggunakan persamaan Dengan : Xij\u000f Kluster ke k p = banyaknya anggota kluster ke - k Code Program: kmeans = KMeans(n_clusters=3, random_state=0).fit(fiturBaru) print(kmeans.labels_) with open('kmeansinopsis.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(kataBaru) for i in fiturBaru: employee_writer.writerow(i) Kesimpulan Shilhoutte Coefficient Metode Shilhoutte Coefficient ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a i . Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b i . Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : S i = (b i \u2013 a i ) / max(a i , b i ) Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif (a i < b i ) dan a i mendekati 0, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1 saat a i = 0. Maka dapat dikatakan, jika s i = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s i = 0 maka objek i*berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s*i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Nilai rata-rata silhoutte coeffisien dari tiap objek dalam suatu cluster adalah suatu ukuran yang menunjukan seberapa ketat data dikelompokan dalam cluster tersebut. Code Program: s_avg = silhouette_score(fiturBaru, kmeans.labels_,random_state=0)","title":"K-Mean Clustering"},{"location":"clustering/#k-mean-clustering","text":"Clustering adalah metode penganalisaan data, yang sering dimasukkan sebagai salah satu metode Data Mining , yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu \u2018wilayah\u2019 yang sama dan data dengan karakteristik yang berbeda ke \u2018wilayah\u2019 yang lain. Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: Tentukan jumlah klaster yang ingin dibentuk dan tetapkan pusat cluster k. Menggunakan jarak euclidean kemudian hitung setiap data ke pusat cluster Kelompokkan data ke dalam cluster dengan jarak yang paling pendek dengan persamaan Hitung pusat cluster yang baru menggunakan persamaan Dengan : Xij\u000f Kluster ke k p = banyaknya anggota kluster ke - k Code Program: kmeans = KMeans(n_clusters=3, random_state=0).fit(fiturBaru) print(kmeans.labels_) with open('kmeansinopsis.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(kataBaru) for i in fiturBaru: employee_writer.writerow(i) Kesimpulan","title":"K Mean Clustering"},{"location":"clustering/#shilhoutte-coefficient","text":"Metode Shilhoutte Coefficient ini berfungsi untuk menguji kualitas dari cluster yang dihasilkan. Metode ini merupakan metode validasi cluster yang menggabungkan metode cohesion dan Separation . Untuk menghitung nilai silhoutte coefisient diperlukan jarak antar dokumen dengan menggunakan rumus EuclideanDistance . Setelah itu tahapan untuk menghitung nilai silhoutte coeffisien adalah sebagai berikut : Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan seluruh objek yang berada dalam satu cluster. Akan didapatkan nilai rata-rata yang disebut a i . Untuk setiap objek i, hitung rata-rata jarak dari objek i dengan objek yang berada di cluster lainnya. Dari semua jarak rata-rata tersebut ambil nilai yang paling kecil. Nilai ini disebut b i . Setelah itu maka untuk objek i memiliki nilai silhoutte coefisien : S i = (b i \u2013 a i ) / max(a i , b i ) Hasil perhitungan nilai silhoutte coeffisien dapat bervariasi antara -1 hingga 1. Hasil clustering dikatakan baik jikai nilai silhoutte coeffisien bernilai positif (a i < b i ) dan a i mendekati 0, sehingga akan menghasilkan nilai silhoutte coeffisien yang maksimum yaitu 1 saat a i = 0. Maka dapat dikatakan, jika s i = 1 berarti objek i sudah berada dalam cluster yang tepat. Jika nilai s i = 0 maka objek i*berada di antara dua cluster sehingga objek tersebut tidak jelas harus dimasukan ke dalam cluster A atau cluster B. Akan tetapi, jika s*i = -1 artinya struktur cluster yang dihasilkan overlapping, sehingga objek i lebih tepat dimasukan ke dalam cluster yang lain. Nilai rata-rata silhoutte coeffisien dari tiap objek dalam suatu cluster adalah suatu ukuran yang menunjukan seberapa ketat data dikelompokan dalam cluster tersebut. Code Program: s_avg = silhouette_score(fiturBaru, kmeans.labels_,random_state=0)","title":"Shilhoutte Coefficient"},{"location":"crawl/","text":"Crawling Proses Crawling Langkah paling awal yang harus kita lakukan sebelum melakukan crawling web adalah mengimport library yang dibutuhkan import requests from bs4 import BeautifulSoup import sqlite3 import csv from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory import skfuzzy as fuzz from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score from math import log10 import numpy as np import warnings Membuat database untuk menyimpan data yang erhasil dicrawl. Database dinamakan \"sinopsis.sqlite\" conn = sqlite3.connect('sinopsis.sqlite') conn.execute('DROP TABLE if exists ARTIKEL') conn.execute('''CREATE TABLE ARTIKEL (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.commit() src = \"http://www.tentangsinopsis.com/\" Website asal yang ingin dicrawl adalah http://www.tentangsinopsis.com/ Jadi, simpan di variable src untuk website target kemudian menentukan ada web yang akan diambil kali ini akan mengambil judul, title, dan isi pada web tersebut. while n <= 2: print(n) try: page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') linkhead = soup.findAll(class_='post') pagination = soup.find(class_='Nav') pag= pagination.getText().split(':')[1].split('\u00bb')[0] pagenum = pagination.findAll('a') numpage = [] for i in pag: if i != ' ': numpage.append(i) print(numpage) for links in linkhead: src = links.find('a')['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find(class_='entry') title = konten.find('h1').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i Untuk dapat melakukan crawling data yang tepat sesuai dengan keinginan, terlebih dahulu harus melakukan inspeksi elemen dengan cara menekan tombol ctrl+i pada saat membuka website tersebut, lalu mencari tag dimana data yang kita inginkan berada. Artikel yang ingin kita crawl berada pada tag <a> soup.find(parameter) digunakan untuk mendapatkan satu tag html yang muncul pertama kali soup.findAll(parameter) digunakan untuk mendapatkan semua tag html tersebut` Untuk mendapatkan textnya, digunakan ` .getText() pada objek BeautifulSoup. Sumber: https://www.dataquest.io/blog/web-scraping-tutorial-python/","title":"Crawling"},{"location":"crawl/#crawling","text":"","title":"Crawling"},{"location":"crawl/#proses-crawling","text":"Langkah paling awal yang harus kita lakukan sebelum melakukan crawling web adalah mengimport library yang dibutuhkan import requests from bs4 import BeautifulSoup import sqlite3 import csv from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory import skfuzzy as fuzz from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score from math import log10 import numpy as np import warnings Membuat database untuk menyimpan data yang erhasil dicrawl. Database dinamakan \"sinopsis.sqlite\" conn = sqlite3.connect('sinopsis.sqlite') conn.execute('DROP TABLE if exists ARTIKEL') conn.execute('''CREATE TABLE ARTIKEL (TITLE TEXT NOT NULL, ISI TEXT NOT NULL);''') conn.commit() src = \"http://www.tentangsinopsis.com/\" Website asal yang ingin dicrawl adalah http://www.tentangsinopsis.com/ Jadi, simpan di variable src untuk website target kemudian menentukan ada web yang akan diambil kali ini akan mengambil judul, title, dan isi pada web tersebut. while n <= 2: print(n) try: page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') linkhead = soup.findAll(class_='post') pagination = soup.find(class_='Nav') pag= pagination.getText().split(':')[1].split('\u00bb')[0] pagenum = pagination.findAll('a') numpage = [] for i in pag: if i != ' ': numpage.append(i) print(numpage) for links in linkhead: src = links.find('a')['href'] page = requests.get(src) soup = BeautifulSoup(page.content, 'html.parser') konten = soup.find(class_='entry') title = konten.find('h1').getText() temp = konten.findAll('p') isi = [] for j in range(len(temp)): isi += [temp[j].getText()] isif = \"\" for i in isi: isif += i Untuk dapat melakukan crawling data yang tepat sesuai dengan keinginan, terlebih dahulu harus melakukan inspeksi elemen dengan cara menekan tombol ctrl+i pada saat membuka website tersebut, lalu mencari tag dimana data yang kita inginkan berada. Artikel yang ingin kita crawl berada pada tag <a> soup.find(parameter) digunakan untuk mendapatkan satu tag html yang muncul pertama kali soup.findAll(parameter) digunakan untuk mendapatkan semua tag html tersebut` Untuk mendapatkan textnya, digunakan ` .getText() pada objek BeautifulSoup. Sumber: https://www.dataquest.io/blog/web-scraping-tutorial-python/","title":"Proses Crawling"},{"location":"kesimpulan/","text":"Kesimpulan Dari program yang telah dijelaskan, dapat disimpulkan bahwa Web crawling merupakan sebuah metode yang sangat efektif untuk mengumpulan data yang berjumlah besar dalam waktu yang sangat singkat . Apalagi data yang di ambil bisa langsung di proses sehingga menjadi sebuah informasi yang utuh dan akurat . Dengan adanya library di dalam Python lebih memudahkan proses komputasi. Jika ingin melakukan clustering, data dari web yang dicrawling harus lumayan banyak agar dapat terlihat hasilnya. Pada program kali ini, pearson correlation dirasa masih kurang karena juga merupakan metode yang sederhana. Diobutuhkan perbandingan dengan metode yang lain untuk mendapatkan hasil yang lebih akurat. Semoga dengan adanya penjabaran ini dapat bermanfaat untuk orang banyak dan digunakan untuk kebaikan.","title":"Kesimpulan"},{"location":"preprocessing/","text":"Preprocessing Sebelum data diproses, pada tahap ini dilakukan ekstraksi teks untuk mendapatkan kata penting. Ada 3 metode yaitu: Tokenisasi (n-gram): memecah kalimat kalimat per kata, seperti \"aku makan sayur\", menjadi \"aku\", \"makan\", \"sayur\". Dalam tokenisasi ini terdapat istilah n-gram, yaitu variasi jumlah kata yang dipecah. Misal dipecah menjadi 2 suku kata, seperti \"aku cinta kepada dia\", menjadi \"aku cinta\", \"kepada dia\". Stopword Removal: menghilangkan kata-kata dan tanda baca yang tidak menjadi fokus search engine karena terlalu sering muncul seperti saya, kamu, dia, dan. Stemming: mengubah suatu kata menjadi kata dasar, seperti kata \"memilih\" menjadi \"pilih\", \"memasangkan\" menajadi \"pasang\". Untuk mempermudah komputasi, digunakan library Python yaitu Sastrawi . stopword = factory.create_stop_word_remover() from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() tmp ='' for i in isi: tmp = tmp+' '+i stop = stopword.remove(tmp) stem = stemmer.stem(stop) tmp = stem.split() katadasar=[] for i in tmp: if not i in katadasar: katadasar.append(i) Untuk melakukan Stopword pertama kita perlu menginisiasi StemmerFactory stopword = factory.create_stop_word_remover() stop = stopword.remove(tmp) Kemudian dilanjutkan dengan melakukan stemming from Sastrawi.Stemmer.StemmerFactory import StemmerFactory factory = StemmerFactory() stemmer = factory.create_stemmer() stopword = factory.create_stop_word_remover() Untuk melakukan tokenisasi, kita bisa menggunakan method String bawaan, yaitu .split() . Kemudian kita akan membuat kondisi untuk melakukan n-gram","title":"Preprocessing"},{"location":"seleksifitur/","text":"Seleksi Fitur Seleksi fitur merupakan salah satu cara untuk mengurangi dimensi fitur yang sangat banyak. Seperti pada kasus kita, Text Mining, jumlah fitur yang didapatkan bisa mencapai lebih dari 2000 kata yang berbeda. Namun, tidak semua kata tersebut benar-benar berpengaruh pada hasil akhir nantinya. Seleksi Fitur juga memiliki banyak sekali metode-metode, seperti Information Gain, Chi Square, Pearson, dll. Kali ini metode yang digunakan yaitu Pearson Correlation Korelasi Pearson merupakan salah satu ukuran korelasi yang digunakan untuk mengukur kekuatan dan arah hubungan linier dari dua veriabel. Dua variabel dikatakan berkorelasi apabila perubahan salah satu variabel disertai dengan perubahan variabel lainnya, baik dalam arah yang sama ataupun arah yang sebaliknya. Harus diingat bahwa nilai koefisien korelasi yang kecil (tidak signifikan) bukan berarti kedua variabel tersebut tidak saling berhubungan . Mungkin saja dua variabel mempunyai keeratan hubungan yang kuat namun nilai koefisien korelasinya mendekati nol, misalnya pada kasus hubungan non linier . Dengan demikian, koefisien korelasi hanya mengukur kekuatan hubungan linier dan tidak pada hubungan non linier. Semakin tinggi nilai koefisien korelasi fitur dengan koefisien batas yang ditentukan maka semakin kuat hubungan fiturnya, begitu pula sebaliknya. Fitur yang memiliki korelasi paling lemah akan dibuang dari data yang ada. Contoh Program: def pearsonCalculate(data, u,v): \"i, j is an index\" atas=0; bawah_kiri=0; bawah_kanan = 0 for k in range(len(data)): atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) bawah_kiri += (data[k,u] - meanFitur[u])**2 bawah_kanan += (data[k,v] - meanFitur[v])**2 bawah_kiri = bawah_kiri ** 0.5 bawah_kanan = bawah_kanan ** 0.5 return atas/(bawah_kiri * bawah_kanan) def meanF(data): meanFitur=[] for i in range(len(data[0])): meanFitur.append(sum(data[:,i])/len(data)) return np.array(meanFitur) def seleksiFiturPearson(katadasar,data, threshold): global meanFitur meanFitur = meanF(data) u=0 while u < len(data[0]): dataBaru=data[:, :u+1] meanBaru=meanFitur[:u+1] kataBaru=katadasar[:u+1] v = u while v < len(data[0]): if u != v: value = pearsonCalculate(data, u,v) if value < threshold: dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) meanBaru = np.hstack((meanBaru, meanFitur[v])) kataBaru = np.hstack((kataBaru, katadasar[v])) v+=1 data = dataBaru meanFitur=meanBaru katadasar=kataBaru #if u%50 == 0 : print(\"proses : \", u, data.shape) u+=1 return data, katadasar fiturBaru, kataBaru = seleksiFiturPearson(katadasar, tfidf, 0.9) with open('selfitursinopsis.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(kataBaru) for i in fiturBaru: employee_writer.writerow(i) Sumber: https://smartstat.wordpress.com/2010/11/21/korelasi-pearson/ https://www.researchgate.net/publication/ https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf","title":"Seleksi Fitur"},{"location":"tfidf/","text":"TF x IDF TF IDF merupakan dua statistik data yang terdiri dari Term-Frequency dan Inverse Data-Frequency . Rumus TF-IDF sendiri terbilang mudah karena hanya TFxIDF. Menghitung TF TF sama dengan VSM yaitu menghitung frekuensi kemunculan kata pada setiap data. Menghitung IDF Nilai kemunculan tiap kata (fitur) berapa kali dalam semua data. df = list() total_doc = bow.shape[0] for kolom in range(len(bow[0])): total = 0 for baris in range(len(bow)): if (bow[baris, kolom] > 0): total +=1 df.append(total) df = np.array(df) idf = list() for i in df: tmp = 1 + log10(total_doc/(1+i)) idf.append(tmp) idf = np.array(idf) tfidf = bow * idf with open('tfidfsinopsis.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in tfidf: employee_writer.writerow(i) Rumus TF IDF sendiri hanya tfidf = bow * idf , dimana nilai IDF didapatkan dari perhitungan 1 + log10(total_doc/(1+i)) Lalu hasil perhitungan dieksport ke dalam bentuk CSV menggunakan program berikut: with open('tfidfsinopsis.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in tfidf: employee_writer.writerow(i)","title":"TFxIDF"},{"location":"tfidf/#tf-x-idf","text":"TF IDF merupakan dua statistik data yang terdiri dari Term-Frequency dan Inverse Data-Frequency . Rumus TF-IDF sendiri terbilang mudah karena hanya TFxIDF. Menghitung TF TF sama dengan VSM yaitu menghitung frekuensi kemunculan kata pada setiap data. Menghitung IDF Nilai kemunculan tiap kata (fitur) berapa kali dalam semua data. df = list() total_doc = bow.shape[0] for kolom in range(len(bow[0])): total = 0 for baris in range(len(bow)): if (bow[baris, kolom] > 0): total +=1 df.append(total) df = np.array(df) idf = list() for i in df: tmp = 1 + log10(total_doc/(1+i)) idf.append(tmp) idf = np.array(idf) tfidf = bow * idf with open('tfidfsinopsis.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in tfidf: employee_writer.writerow(i) Rumus TF IDF sendiri hanya tfidf = bow * idf , dimana nilai IDF didapatkan dari perhitungan 1 + log10(total_doc/(1+i)) Lalu hasil perhitungan dieksport ke dalam bentuk CSV menggunakan program berikut: with open('tfidfsinopsis.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in tfidf: employee_writer.writerow(i)","title":"TF x IDF"},{"location":"viesem/","text":"Vector Space Model Dalam VSM, koleksi dokumen direpresentasikan sebagai sebuah matrik term document (atau matrik term frequency ). Setiap sel dalam matrik bersesuaian dengan bobot yang diberikan dari suatu term dalam dokumen yang ditentukan. Nilai nol berarti bahwa term tersebut tidak ada dalam dokumen. Dalam proses VSM terjadi perhitungan frekuensi kemunculan setiap kata pada setiap data yang telah dicrawl lalu disusun dalam bentuk matriks. Setiap sel dalam matrik bersesuaian dengan bobot yang diberikan dari suatu term dalam dokumen yang ditentukan. Nilai nol berarti bahwa term tersebut tidak ada dalam dokumen. Contoh: Aku cinta kamu Tapi kamu cinta dia No. Aku Cinta Kamu Tapi Dia 1. 1 1 1 0 0 2. 0 1 1 1 1 Code Program cursor = conn.execute(\"SELECT * from ARTIKEL\") for row in cursor: tampung = [] for i in katadasar: tampung.append(row[1].lower().count(i)) matrix.append(tampung) for i in matrix: print(i) Pada code diatas, dilakukan proses pengambilan data dari database, lalu dilakukan penghitungan frekuensi kemunculan kata dasar yang disimpan dalam bentuk matriks. Lalu matriks tersebut diimpor ke dalam bentuk CSV agar lebih memudahkan pengguna untuk membaca data. Berikut program untuk mengimport hasil VSM ke dalam bentuk CSV: with open('tentangsinopsis.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in matrix: employee_writer.writerow(i)","title":"VSM"},{"location":"viesem/#vector-space-model","text":"Dalam VSM, koleksi dokumen direpresentasikan sebagai sebuah matrik term document (atau matrik term frequency ). Setiap sel dalam matrik bersesuaian dengan bobot yang diberikan dari suatu term dalam dokumen yang ditentukan. Nilai nol berarti bahwa term tersebut tidak ada dalam dokumen. Dalam proses VSM terjadi perhitungan frekuensi kemunculan setiap kata pada setiap data yang telah dicrawl lalu disusun dalam bentuk matriks. Setiap sel dalam matrik bersesuaian dengan bobot yang diberikan dari suatu term dalam dokumen yang ditentukan. Nilai nol berarti bahwa term tersebut tidak ada dalam dokumen. Contoh: Aku cinta kamu Tapi kamu cinta dia No. Aku Cinta Kamu Tapi Dia 1. 1 1 1 0 0 2. 0 1 1 1 1 Code Program cursor = conn.execute(\"SELECT * from ARTIKEL\") for row in cursor: tampung = [] for i in katadasar: tampung.append(row[1].lower().count(i)) matrix.append(tampung) for i in matrix: print(i) Pada code diatas, dilakukan proses pengambilan data dari database, lalu dilakukan penghitungan frekuensi kemunculan kata dasar yang disimpan dalam bentuk matriks. Lalu matriks tersebut diimpor ke dalam bentuk CSV agar lebih memudahkan pengguna untuk membaca data. Berikut program untuk mengimport hasil VSM ke dalam bentuk CSV: with open('tentangsinopsis.csv', mode='w') as employee_file: employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) employee_writer.writerow(katadasar) for i in matrix: employee_writer.writerow(i)","title":"Vector Space Model"}]}